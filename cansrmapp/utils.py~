import torch
import time
import sys
from . import DEVICE
from . import np

def _tcast(arg): 

    if torch.is_tensor(arg) : 
        if torch.get_device(arg) != DEVICE : 
            return arg.to(DEVICE)
        return arg
    elif type(arg) == list : 
        if torch.is_tensor(arg[0]) : 
            return torch.tensor(np.array([a.cpu() for a in arg]),device=DEVICE)
        else : 
            return torch.tensor(np.array(arg),device=DEVICE)
    else : 
        return torch.tensor(arg,device=DEVICE)

def word_to_seed(word) :
    import hashlib
    return int(hashlib.shake_128(word.encode()).hexdigest(4),16)

def msg(*args,**kwargs) : 
    print(time.strftime("%m%d %H:%M:%S",time.localtime())+'|',*args,**kwargs)
    sys.stdout.flush()

def diag_embed(thetensor,minorly=False) :

    if not minorly :
        use_indices=thetensor.indices().numpy()
        outindices=np.stack([use_indices[0],*use_indices],axis=0)
        outshape=(thetensor.shape[0],*tuple(thetensor.shape))
    else : 
        use_indices=thetensor.indices().numpy()
        outindices=np.stack([*use_indices,use_indices[-1]],axis=0)
        outshape=(*tuple(thetensor.shape),thetensor.shape[-1],)

    return torch.sparse_coo_tensor(indices=outindices,values=thetensor.values(),size=outshape).coalesce()

def mask_sparse_rows(t,mask) :
    """
    Utility function. Takes sparse tensor t and returns the same with only rows marked True in mask
    Mask is also a tensor and must be on the same device
    """
    imi=torch.argwhere(mask).ravel()
    new_indices=torch.cumsum(mask,0)-1
    timask=torch.isin(t.indices()[0],imi)
    stvals=t.values()[timask]
    ti=t.indices()[:,timask]
    stindices=torch.stack([new_indices[ti[0]],ti[1]],axis=-1).transpose(0,1)
    st=torch.sparse_coo_tensor(
        values=stvals,
        indices=stindices,
        size=(mask.sum(),t.shape[1]),
        device=t.device,
    )

    return st

def mask_sparse_columns(t,mask) : 
    """
    Utility function. Takes sparse tensor t and returns the same with only columns marked True in mask
    Mask is also a tensor and must be on the same device
    """
    imi=torch.argwhere(mask).ravel()

    new_indices=torch.cumsum(mask,0)-1
    
    timask=torch.isin(t.indices()[1],imi)

    stvals=t.values()[timask]
    
    ti=t.indices()[:,timask]

    stindices=torch.stack([ti[0],new_indices[ti[1]]],axis=-1).transpose(0,1)
    
    st=torch.sparse_coo_tensor(
        values=stvals,
        indices=stindices,
        size=(t.shape[0],mask.sum()),
        device=t.device,
    )

    return st


def _fix_overlong_identifiers(df,index=True) :
    """
    TCGA sample identifiers will specifiy particular specimens, aliquots, etc
    This reduces identifiers to the first three items, which uniquely specify
    only the patient, and no other features of the sample.
    """
    if index: 
        df.index=[ '-'.join(x.split('-')[:3]) for x in df.index ]
        assert len(df.index) == len(set(df.index))
    else : 
        df.columns=[ '-'.join(x.split('-')[:3]) for x in df.columns ]
        assert len(df.columns) == len(set(df.columns))

    return df

def regularize_cc(ccmat,n,correlation_p=1e-3) : 
    """
    Takes a correlation matrix (numpy ndarray) and zeroes
    out correlations with p-values less significant than <correlation_p>
    """
    tstats=ccmat*np.sqrt(n-2)/np.sqrt(1-ccmat*ccmat)
    tthresh=t.isf(correlation_p,df=n-2)
    sigmask=tstats > tthresh
    out=ccmat.copy()
    out[ ~sigmask ]=0
    
    return out

def regularize_cc_torch(ccten,n,correlation_p=1e-3) : 
    """
    Takes a correlation matrix (torch.Tensor) and zeroes
    out correlations with p-values less significant than <correlation_p>
    """
    from scipy.stats.distributions import t
    tstats=ccten*torch.sqrt(torch.tensor(n,device=ccten.device)-2)/torch.sqrt(1-ccten*ccten)
    tthresh=t.isf(correlation_p,df=n-2)
    sigmask=tstats > tthresh
    out=ccten.clone()
    out[ ~sigmask ]=0
    
    return out

def cc2mats(m1,m2) :
    """
    This function finds the correlation coefficient between all **columns** of two matrices m1 and m2.
    (The rows are the observations measured across all variables in both columns).
    This does _not_ show the correlation coefficient between any two variables that are both columns of the 
    same matrix.
    """

    from scipy.stats.distributions import t
    m1bar=m1.mean(axis=0)
    m2bar=m2.mean(axis=0)
    m1sig=m1.std(axis=0)
    m2sig=m2.std(axis=0)
    n=m1.shape[0]

    cov=np.dot(
            (m1-m1bar).transpose(),
            (m2-m2bar)
            )
    with warnings.catch_warnings() : 
        warnings.simplefilter('ignore')  
        cov1=cov/m2sig
        cov1[ np.isnan(cov1) ]=0
        cov2=cov1.transpose()/m1sig
        cov2[ np.isnan(cov2) ]=0
        out=cov2.transpose()/n

    return out

def cc2tens(t1,t2) : 
    """
    This function finds the correlation coefficient between all **columns** of two dense tensors t1 and t2.
    (The rows are the observations measured across all variables in both columns).
    This does _not_ show the correlation coefficient between any two variables that are both columns of the 
    same matrix.
    """
    t1=t1.cpu()
    t2=t2.cpu()
    if t1.is_sparse: 
        t1=t1.clone().to_dense()
    if t2.is_sparse: 
        t2=t2.clone().to_dense()


    t1bar=t1.mean(axis=0)
    t2bar=t2.mean(axis=0)
    t1sig=t1.std(axis=0)
    t2sig=t2.std(axis=0)
    n=t1.shape[0]

    cov=torch.matmul(
            (t1-t1bar).transpose(0,1),
            (t2-t2bar)
            )
    cov1=cov/t2sig
    cov1[ torch.isnan(cov1) ]=0
    cov2=cov1.transpose(0,1)/t1sig
    cov2[ torch.isnan(cov2) ]=0
    out=cov2.transpose(0,1)/n
    return out


def flatten_minorly(thetensor) : 
    """
    Flattens sparse tensor thetensor maintaining the last (-1th) axis and flattening the remainder
    """

    use_indices=thetensor.indices().cpu().numpy()
    use_shape=thetensor.shape

    outsize=(np.prod(use_shape[:-1]),use_shape[-1])
    ri=np.ravel_multi_index(use_indices[:-1],dims=use_shape[:-1])
    maj_i=use_indices[-1]

    outindices=np.stack([ri,maj_i],axis=0)

    return torch.sparse_coo_tensor(indices=outindices,values=thetensor.values(),size=outsize).coalesce()
    
def sproing(theflattensor,minor_dims,minorly=False) :
    """
    Unflattens a flat tensor so that its second dimension (indexed 1) is
    distributed among the dimensions listed in minor_dims.
    ^^ If "minorly", otherwise so that first dimension is distributed among minor dims.
    """

    if minorly : 
        inindices=theflattensor.indices().cpu().numpy()
        major=inindices[0]
        rest=inindices[1]

        #print(theflattensor.shape[0]*np.prod(minor_dims),np.prod(theflattensor.shape))
        assert theflattensor.shape[0]*np.prod(minor_dims)==np.prod(theflattensor.shape)

        sproinged_rest=np.unravel_index(rest,shape=minor_dims)

        outindices=np.stack([major,*sproinged_rest],axis=0)
        outsize=(theflattensor.shape[0],*minor_dims)

    else : 
        inindices=theflattensor.indices().numpy()
        major=inindices[1]
        rest=inindices[0]

        #print(theflattensor.shape[0]*np.prod(minor_dims),np.prod(theflattensor.shape))
        assert theflattensor.shape[1]*np.prod(minor_dims)==np.prod(theflattensor.shape)

        sproinged_rest=np.unravel_index(rest,shape=minor_dims)

        outindices=np.stack([*sproinged_rest,major],axis=0)
        outsize=(*minor_dims,theflattensor.shape[1])
    
    return torch.sparse_coo_tensor(indices=outindices,
        values=theflattensor.values(),
        size=outsize).coalesce()



